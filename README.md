# ai_temp_monitor

## The structure

The script `main.py` runs two functions asynchronously: `stream_data` and `plot_temp`. 

- **`plot_temp`**: Plots a graph of the temperature data over time.
- **`stream_data`**: Performs three tasks:
  1. Simulates the streaming of temperature data coming from a sensor.
  2. Checks whether the temperature data lies between specified lower and upper thresholds.
  3. Writes a warning message to a log file whenever the data falls outside the acceptable range.
  4. Trigger an **AI agent** whenever a new warning is generated.

The agent will read the warnings log file, gather relevant information through the provided tools, and attempt to provide a possible explanation for the temperature variation.


## The structure

The scripts `main.py` runs two functions asynchronously: `stream_data` and `plot_tep`. `plot_temp` plots a graph of the temperature data over time. `stream_data` does three things, it simulate the streaming of temperature data coming from a sensor, it checks if the temperature data lies in between a lower and upper thresholds and write a warning message on a warning log file each time it is not the case. Whenever a new warning is generated the agent is called in another thread. The agent will read the warnings log file, gather information through the help of provided tools, and try to give a possible explanation of the temperature variation.  


## AI agent

The AI agent's task is to provide possible explanations for temperature variations. It accomplishes this by gathering information and analyzing it using the tools it has been provided with.
The agent consists of an orchestrator, `orchestrator.py`, and the two tools: `week_avg_temp_tool.py` and `writing_tool.py`.
The orchestrator uses `week_avg_temp_tool.py` to gather weather information of the week, and then process this data with the `writing_tool.py` which is responsible for generating a summary of detected warnings, analyzing the information and offering a possible explanation for the temperature variations.

The tool framework is defined in `base_tool.py`, it is designed to be flexible, making it easy to add new tools to the orchestrator as needed, which is the intended goal.

The orchestrator utilizes an LLM, specifically gemini-2.0-flash. 
The tool that gathers the weekly average temperature makes use of a free API from [weatherapi](https://www.weatherapi.com/)
The writing tool that generates the summary and explanation is also powered by gemini-2.0-flash.

### Possible Improvements

Ideally, the orchestrator should have more tools. For example, a tool that retrieves manufacturing information about the sensor, a tool that has access to the sensor's location data, etc.
I think it becomes interesting when the tool can process large amounts of data that would take a person much longer to go through.

For the LLM, I used **gemini-2.0-flash** (because I had a free API key for two months). 
However, I believe it would be more cost-effective and efficient to use a smaller local model. When used inside a tool, such a model could be fine-tuned with specific data.


## The dataset and the streaming

The data are mock data obtain from [ kaggle ](https://www.kaggle.com/datasets/garystafford/environmental-sensor-data-132k).
These are mock data used to build the application.
Of all the different data types of the dataset I used only the temperature data. The timestamp data are generated by the program when the temperature data of the dataset are generated.

The dataset is first preprocessed and sucessively read by the a generator function defined in `device_stream.py`.

